# 快速数据迁移
顾名思义，这是一个将数据从一个数据库迁移到另一个数据库的工具。与很多同类型工具相比，它主打的特点是“快”！
没错，就是
### “快”！
[========]
## 为什么“快”？
要解释这个问题，首先要说明白数据迁移是什么一回事。

数据迁移通常是这样子的：
```flow
srcdb=>operation: 源库
ret=>operation: 提取数据（JDBC, ODBC...）
con=>operation: 转换目标格式
destdb=>operation: 目标库

srcdb->ret->con->destdb
```
----
如果通用些，可能就类似这个样子了：
```flow
srcdb=>operation: 源库
ret=>operation: 提取数据（reader）
con1=>operation: 转换中间格式
con2=>operation: 转换目标格式(writer)
destdb=>operation: 目标库

srcdb->ret->con1->con2->destdb
```
----
这是一个串行处理的过程，提取数据的时候，目标库空闲着，写入数据的时候，源库空闲着，处理能力白白浪费了。
可见，只要将这个串行处理改为并行，数据迁移效率即可大大提高，在某些测试场景下，这项小小的改动，可以提升 30% 的迁移效率！
```seq
source->reader: 提取数据
reader->writer: 中间格式
writer->storage: 目标格式
storage-->writer: 异步获取
writer->dest: 写入
```
## 制约迁移效率的其它主要因素
除了执行机制的制约以外，还有其它一些制约因素会明显影响迁移效率。
### 1、网络传输速度
显然，如果只有一根小水管，算法层面优化能带来的提升空间就不大了，不过也不是什么事情都不能做，比如 MySQL 的客户端就支持压缩传输协议，只需一个连接串的配置项，就能用 CPU 运算能力换来一定的效率提升。
### 2、中间格式转换
对于立足于“通用”的解决方案，在 reader 和 writer 之间引入中间数据格式貌似无可避免，这令中间格式的选择变得非常重要，JSON，或者 XML，通用性自然无可质疑，代价却不小。
### 3、目标库写入效率
如果目标库客户端提供 Bulk Copy 支持，使用批量复制接口，写入性能自然更佳，在某些测试场景下，批量复制比对数据脚本，效率可提升 20 倍，没写错，是 20 倍！（因为是未经严谨优化的测试场景，就不点名了，哈哈！）

----
### 未完待续...
